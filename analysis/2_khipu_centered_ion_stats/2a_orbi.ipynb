{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get stats on ions based on mz delta to khipu primary peaks\n",
    "\n",
    "For 45 Orbitrap datasets from the CSM project. Use delta to khipu primary peaks, no need for full pairwise (see Step 1).\n",
    "\n",
    "We take histogram counts per dataset, as KDE is not easy to automate on less dense data of varying quality. Peak detection is done on cumulative histogram.\n",
    "\n",
    "This produces most frequent mass delta values for pos and neg ionization on Orbitraps.\n",
    "\n",
    "SL 2024-11-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import tqdm\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks \n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import statsmodels.api as sm\n",
    "from khipu.extended import peaklist_to_khipu_list, export_empCpd_khipu_list\n",
    "import importlib\n",
    "sys.path.insert(0, '..')\n",
    "from mining import * \n",
    "from isf_helper import calculate_bin_deltas_fulldataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, os\n",
    "download_path = '../input_data_orbi.zip'\n",
    "extract_path = '..'\n",
    "\n",
    "# download from zenodo take 20sec-2min. Only need to do once. \n",
    "try:\n",
    "    with requests.get('https://zenodo.org/records/14541717/files/isf_input_data_orbi.zip?download=1', stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(download_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "    print(f\"Downloaded file to: {os.path.abspath(download_path)}\")\n",
    "\n",
    "    with zipfile.ZipFile(download_path, 'r') as z:\n",
    "            z.extractall(path=extract_path)\n",
    "            print(f\"Extracted files to: {os.path.abspath(extract_path)}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error during download: {e}\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(\"The file is not a valid zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    if os.path.exists(download_path):\n",
    "        os.remove(download_path)\n",
    "        print(f\"Deleted downloaded file: {os.path.abspath(download_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbi_datasets = [x.rstrip() for x in open('selected_45_orbi_datasets.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isotope_elution_window(good_khipus):\n",
    "    '''good_khipus must have M0 as a good peak (snr>=5, shape>=0.9)\n",
    "    \n",
    "    good_khipus: a list of khipus\n",
    "\n",
    "    return: Mean and stdev of M0 peak width; mean and stdev of (M1-M0 retention time shift)\n",
    "    '''\n",
    "    M0wdiths, M1shifts = [], []\n",
    "    for epd in good_khipus:\n",
    "        M0, M1 = get_M0(epd['MS1_pseudo_Spectra']), get_M1(epd['MS1_pseudo_Spectra'])\n",
    "        M0wdiths.append(M0['right_base'] - M0['left_base'])\n",
    "        M1shifts.append(M1['rtime'] - M0['rtime'])\n",
    "        \n",
    "    mean_m0, stdev_m0 = np.mean(M0wdiths), np.std(M0wdiths)\n",
    "    mean_shift, stdev_shift = np.mean(M1shifts), np.std(M1shifts)\n",
    "\n",
    "    return mean_m0, stdev_m0, mean_shift, stdev_shift\n",
    "\n",
    "def summarize_dataset(f):\n",
    "    '''called different functions to generate necessary information for following calculation\n",
    "    \n",
    "    f: the name of dataset. Ex, ST001237_HILICpos_B2_ppm5_3524314\n",
    "    \n",
    "    return: stdev of M1-M0 retention time shift, full list of features and list of good khipus\n",
    "    '''\n",
    "    \n",
    "    def path_constructor(f):\n",
    "        '''constructor to generate ion mode and paths for input files\n",
    "        \n",
    "        f: the name of dataset. Ex, ST001237_HILICpos_B2_ppm5_3524314\n",
    "        \n",
    "        return: ion mode, full feature table and preferred feature table\n",
    "        '''\n",
    "        # specific for this analysis\n",
    "        ion_mode = 'pos' if 'pos' in f else 'neg'\n",
    "        full_feature_table = f'../input_data_orbi/{f}/full_feature_table.tsv'\n",
    "        preferred_feature_table = f'../input_data_orbi/{f}/preferred_feature_table.tsv'\n",
    "        \n",
    "        return ion_mode, full_feature_table, preferred_feature_table\n",
    "\n",
    "    def generate_ecpds_from_feature_path(path, ion_mode):\n",
    "        '''generate empirical compounds json from the path of full feature table\n",
    "        \n",
    "        path: the path to full feature table\n",
    "        ion_mode: 'pos' or 'neg'\n",
    "        \n",
    "        returns: the path to the generated ecpds.json. Will be under input_data_orbi/{f}/. f is the current dataset name.\n",
    "        '''\n",
    "        KCD_formula_coordinate = build_KCD_from_formula_coordinate(formula_coordinate)\n",
    "        num_samples, st = read_features_from_asari_table(open(path).read())\n",
    "        mass_accuracy_ratio, st = custom_mz_calibrate(st, KCD_formula_coordinate, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmode=ion_mode, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmz_tolerance_ppm=5, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trequired_calibrate_threshold=0.000002)\n",
    "        _json_empcpd = os.path.join(f'../input_data_orbi/{f}/ecpds.json')\n",
    "        features2cpds(st, mode=ion_mode, mz_tolerance_ppm=5, rt_tolerance=2, outfile=_json_empcpd)\n",
    "        return _json_empcpd\n",
    "    \n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        ion_mode, full_Feature_table, _ = path_constructor(f)\n",
    "        if not os.path.exists(f'../input_data_orbi/{f}/ecpds.json'):\n",
    "            ecpds_path = generate_ecpds_from_feature_path(full_Feature_table, ion_mode)\n",
    "        else:\n",
    "            ecpds_path = f'../input_data_orbi/{f}/ecpds.json'\n",
    "        _, _, epd_summary = epd2featurelist_from_file(ecpds_path, mode=ion_mode)\n",
    "        _, _, _, stdev_shift = get_isotope_elution_window(epd_summary['good_khipus'])\n",
    "        _, featureList = read_features_from_asari_table(open(full_Feature_table).read())\n",
    "    return stdev_shift, featureList, epd_summary['good_khipus']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mass Deltas Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [16:21<00:00, 21.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# ~1h for first time. Once ecpds generated, <20min\n",
    "collection_deltas = [] # the list of mz delta value list of each dataset\n",
    "for orbi_dataset in tqdm.tqdm(orbi_datasets):\n",
    "    stdev_shift, st, good_khipus = summarize_dataset(orbi_dataset)\n",
    "    mz_delta_list = calculate_bin_deltas_fulldataset(good_khipus, stdev_shift, st)\n",
    "    collection_deltas.append(mz_delta_list)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explained features by intensity quartiles\n",
    "\n",
    "Get khipus using updated grid,\n",
    "Add new list of fragments\n",
    "- Collect features that are explained;\n",
    "- and numbers of unique empCpds\n",
    "\n",
    "Plot how they overlap by each incremental quartile of features. (Done in step5)\n",
    "\n",
    "The candidate fragments are based on frequent delta m/zs. \n",
    "\n",
    "Not using those from MS2 as the freq lists here should be more inclusive.\n",
    "\n",
    "Will move the frequent lists and top ones into mass2chem and khipu packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Elution Parameters for Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "elution_parameters = []\n",
    "for f in orbi_datasets:\n",
    "    _lf, f2epd, epd_summary = epd2featurelist_from_file(f'../input_data_orbi/{f}/ecpds.json',  mode='pos' if 'pos' in f else 'neg')\n",
    "    elution_parameters.append((f, epd_summary['num_good_khipus'], get_isotope_elution_window(epd_summary['good_khipus'])))\n",
    "    \n",
    "s = 'ID\\tnumber_good_khipus\\tmean_m0\\tstdev_m0\\tmean_shift\\tstdev_shift\\n'    \n",
    "# flatten the list\n",
    "for ep in elution_parameters:\n",
    "    s += '\\t'.join([str(round(item, 3) if isinstance(item, float) else item) for sublist in ep for item in (sublist if isinstance(sublist, tuple) else (sublist,))]) + '\\n'    \n",
    "        \n",
    "with open('elution_parameters_45studies_orbi.tsv', 'w') as O:\n",
    "    O.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Histogram and KDE for Each Dataset\n",
    "The result will be under freq_mzdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram_per_dataset(list_kp_deltas, binsize=0.001):\n",
    "    list_deltas = []\n",
    "    for v in list_kp_deltas:\n",
    "        list_deltas += v[1]\n",
    "    list_deltas = np.array(list_deltas)\n",
    "    _min, _max = list_deltas.min(), list_deltas.max()\n",
    "    bins = np.arange(_min, _max, binsize)\n",
    "    hist = np.histogram(list_deltas, bins)\n",
    "    return hist\n",
    "\n",
    "def write_top_histo_bins(f, list_kp_deltas, binsize=0.001, outdir='../freq_mzdelta_orbi/', topN=500):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    h1, b2 = get_histogram_per_dataset(list_kp_deltas, binsize=binsize)\n",
    "    new = list(zip(h1, b2))\n",
    "    new.sort(reverse=True)\n",
    "    s = 'count\\tbin_left_edge\\n'\n",
    "    for x in new[:topN]:\n",
    "        s += str(x[0]) + '\\t' + str(round(x[1], 4)) + '\\n'\n",
    "    with open(outdir+'histo_'+f+'.tsv', 'w') as O:\n",
    "        O.write(s)\n",
    "\n",
    "def write_top_kde_peaks(f, peaks_density, outdir='../freq_mzdelta_orbi/'):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    \n",
    "    # peaks_density : [(mz, density), ...]\n",
    "    s = 'mz_peak\\tKDE_density\\n'\n",
    "    for x in peaks_density:\n",
    "        s += str(round(x[0], 4)) + '\\t' + str(round(x[1], 4)) + '\\n'\n",
    "    with open(outdir+'kde_'+f+'.tsv', 'w') as O:\n",
    "        O.write(s)\n",
    "\n",
    "def get_peaks(x_hist, y_hist, \n",
    "                  height=0.01,\n",
    "                  distance=2,\n",
    "                  # prominence=0.5,\n",
    "                  width=2,\n",
    "                  wlen=50,\n",
    "                  ):\n",
    "    # prominence is not used as height is sufficient after smoothing first\n",
    "    y_hist_smoothed = uniform_filter1d(y_hist, 5, mode='nearest')\n",
    "    peaks, properties = find_peaks(y_hist_smoothed, \n",
    "                                    height=height, \n",
    "                                    distance=distance,\n",
    "                                    # prominence=prominence,\n",
    "                                    width=width, \n",
    "                                    wlen=wlen,\n",
    "                                    ) \n",
    "    real_apexes = [x_hist[ii] for ii in peaks]\n",
    "    return list(zip(real_apexes, properties['peak_heights']))\n",
    "\n",
    "def get_kde_per_dataset(list_kp_deltas, bandwidth=0.001, threshold=0.001, topN=500):\n",
    "    '''\n",
    "    Testing. Not using in final results.\n",
    "    '''\n",
    "    list_deltas = []\n",
    "    for v in list_kp_deltas:\n",
    "        list_deltas += v[1]\n",
    "    kde = sm.nonparametric.KDEUnivariate(list_deltas)\n",
    "    kde.fit(bw=bandwidth) \n",
    "    max_intensity = kde.density.max()\n",
    "    threshold = min(threshold, threshold*max_intensity)\n",
    "    # prominence = 0.05 * max_intensity\n",
    "    peaks_density = get_peaks(kde.support, kde.density, height=threshold, \n",
    "                                        # prominence=prominence,\n",
    "                                        )\n",
    "    peaks_density = sorted(peaks_density, key=lambda x: x[1], reverse=True)\n",
    "    return peaks_density[:topN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for orbi_dataset, deltas_per_dataset in zip(orbi_datasets, collection_deltas): \n",
    "    # do histogram\n",
    "    write_top_histo_bins(orbi_dataset, deltas_per_dataset)    \n",
    "    \n",
    "    # do KDE\n",
    "    peaks_density = get_kde_per_dataset(deltas_per_dataset, bandwidth=0.0005, threshold=0.001, topN=500)\n",
    "    write_top_kde_peaks(orbi_dataset, peaks_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Tricky to optimize KDE peak finding parameters for noisier data. When overall density is low, peak shapes are not clear.\n",
    "\n",
    "Histogram is robust to use here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cumulative Histogram and KDE across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram_fixed_bins(list_kp_deltas, fixed_bins):\n",
    "    '''Returns histogram on fixed bins using abs values\n",
    "    \n",
    "    list_kp_deltas: list of tuples containing khipu id and related list of delta mz values. \n",
    "        Ex, [('kp4_69.0587', [0.0,\n",
    "                5.033599999999993,\n",
    "                22.167199999999994,\n",
    "                11.455299999999994,\n",
    "                1.0032999999999959]),\n",
    "            ('kp5_69.0587', [0.0,\n",
    "                10.871099999999998,\n",
    "                1.0032999999999959])]\n",
    "    fixed_bins: an 1D numpy array of evenly spaced floating-point numbers. Ex. np.arange(0.1, 310, 0.0001)\n",
    "    \n",
    "    return: 1D numpy array representing counts of all deltas in current dataset in each bin given by fixed_bins. Ex. array([22.,  6., 14., ...,  7.,  5.,  4.], shape=(3098999,))\n",
    "    '''\n",
    "    list_deltas = []\n",
    "    for v in list_kp_deltas:\n",
    "        list_deltas += v[1]\n",
    "    np_list_deltas = abs(np.array(list_deltas))\n",
    "    return np.histogram(np_list_deltas, fixed_bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_bins = np.arange(0.1, 310, 0.0001)\n",
    "cummulative_histo = np.zeros(len(fixed_bins)-1)\n",
    "\n",
    "for d in collection_deltas:\n",
    "    cummulative_histo += get_histogram_fixed_bins(d, fixed_bins)\n",
    "    \n",
    "cummulative_kde = get_peaks(fixed_bins[:-1], cummulative_histo, height=100)\n",
    "cummulative_kde = sorted(cummulative_kde, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate Mass Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mass2chem.lib.common_mass import mass_signatures\n",
    "# remove F\n",
    "mass_signatures = [ms for ms in mass_signatures if 'F' not in ms[2]]\n",
    "\n",
    "def search_mass_signatures(m, mass_signatures=mass_signatures):\n",
    "    '''find the closest mass signatures for the input mass value\n",
    "    \n",
    "    m: float. The input mass value\n",
    "    mass_signatures: list of tuples containing mass value, chem formula, single element dict of each mass signature. \n",
    "    \n",
    "    return: the closest signature and the deviation.\n",
    "        \n",
    "    example:\n",
    "    >>> search_mass_signatures(28.0312)\n",
    "    ((28.0313, '± C2H4, natural alkane chains such as fatty acids', {'C': 2, 'H': 4}), 0.00010000000000331966)\n",
    "    '''\n",
    "    _d = [abs(m-abs(x[0])) for x in mass_signatures]\n",
    "    # find all mass signatures whose notes are not empty strings.\n",
    "    _iis = [i for i, delta in enumerate(_d) if delta == min(_d)]\n",
    "    if _d[_iis[0]] < 0.0005:\n",
    "        if len(_iis) > 1:\n",
    "            if filtered_iis := [_i for _i in _iis if (mass_signatures[_i][1] != '' and mass_signatures[_i][0] > 0)]:\n",
    "                _ii = filtered_iis[0]\n",
    "            else:\n",
    "                _ii = _iis[0]\n",
    "        else:\n",
    "            _ii = _iis[0]\n",
    "        return mass_signatures[_ii], _d[_ii]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def write_delta_mzs(output_path, mz_delta_kdes, mass_signatures):\n",
    "    '''write the top frequent delta mzs into designated tsv file\n",
    "    \n",
    "    output_path: the path of top frequent delta mzs\n",
    "    mz_delta_kdes: list of tuples containing mz delta values and kdes. \n",
    "                    Ex,[(1.0033000000000258, 8393.8),\n",
    "                        (14.015500000000399, 1590.6),\n",
    "                        (2.015500000000055, 1285.8),\n",
    "                        (18.010500000000516, 1244.8)]\n",
    "    mass_signatures: list of tuples containing mass value, chem formula, single element dict of each mass signature. \n",
    "    '''\n",
    "    s = 'delta_mz\\tcount_estimate\\tmass_signature\\tnote\\tdict\\n'\n",
    "    for x in mz_delta_kdes:\n",
    "        _M = search_mass_signatures(x[0], mass_signatures)\n",
    "        if _M:\n",
    "            s += str(round(x[0], 4)) + '\\t' + str(int(x[1])) + '\\t' + '\\t'.join(\n",
    "                [str(ii) for ii in _M[0] ]) + '\\n'\n",
    "        else:\n",
    "            s += str(round(x[0], 4)) + '\\t' + str(int(x[1])) + '\\n'\n",
    "            \n",
    "    with open(output_path, 'w') as O:\n",
    "        O.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_msig = []\n",
    "for x in cummulative_kde:\n",
    "    _M = search_mass_signatures(x[0])\n",
    "    if _M:\n",
    "        curated_msig.append((float(x[0]), _M))\n",
    "        \n",
    "write_delta_mzs('top_frequent_delta_mz_orbi_anno-20250128.tsv', cummulative_kde, curated_msig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(curated_msig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do pos and neg ions separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Remove the module from sys.modules\n",
    "sys.modules.pop('mass2chem.lib.common_mass', None)\n",
    "# Re-import the module\n",
    "from mass2chem.lib.common_mass import mass_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_bins = np.arange(0.1, 500.01, 0.0001)\n",
    "cummulative_histo_pos = np.zeros(len(fixed_bins)-1)\n",
    "cummulative_histo_neg = np.zeros(len(fixed_bins)-1)\n",
    "\n",
    "for orbi_dataset, deltas_per_dataset in zip(orbi_datasets, collection_deltas): \n",
    "    if 'pos' in orbi_dataset:\n",
    "        cummulative_histo_pos += get_histogram_fixed_bins(deltas_per_dataset, fixed_bins)\n",
    "    elif 'neg' in orbi_dataset:\n",
    "        cummulative_histo_neg += get_histogram_fixed_bins(deltas_per_dataset, fixed_bins)\n",
    "    else:\n",
    "        print(\"Error, \", orbi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos\n",
    "peaks_density_pos = get_peaks(fixed_bins[:-1], cummulative_histo_pos, height=100)\n",
    "peaks_density_pos = sorted(peaks_density_pos, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "write_delta_mzs('top_frequent_delta_mz_orbi_pos-20250128.tsv', peaks_density_pos, mass_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg\n",
    "peaks_density_neg = get_peaks(fixed_bins[:-1], cummulative_histo_neg, height=100)\n",
    "peaks_density_neg = sorted(peaks_density_neg, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "write_delta_mzs('top_frequent_delta_mz_orbi_neg-20250128.tsv', peaks_density_neg, mass_signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have calculated most frequent mz deltas in Orbitrap datasets.\n",
    "\n",
    "Rough annotation is drawn from mass2chem etc. \n",
    "\n",
    "Will reformat the tables and update with full annotation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
